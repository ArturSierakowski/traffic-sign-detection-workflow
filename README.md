# Traffic Sign Detection

This module contains scripts for downloading, processing, and training
a custom YOLO11 model for traffic sign detection using imagery from
Mapillary. It also supports experiment tracking with MLflow.

## 👤 Who is this for?

- Just want to use the [app](https://github.com/Shadoweee77/VisiSign)? You don't need this folder. The pretrained model is enough.

- Want to retrain the model or use custom data? Follow the steps below.

## 📦 Technologies Used
- Python 3.12
- PyTorch (CUDA 12.6)
- Labelme
- Albumentations
- Ultralytics YOLO11
- MLflow

> [!NOTE]
> This workflow was tested with Python 3.12 only

## 🗂️ Folder Structure
```bash
traffic-sign-detection-workflow/
├── downloader/            # Mapillary data downloading
├── pipelines/             # Pipeline for preprocessing and dataset structuring
├── preprocessing/         # Additional transformation tools
├── training/              # YOLO training scripts
├── utils/                 # Helper functions
├── README.md              # You are reading it
├── prediction_pytorch.py  # Run detection on video
└── requirements.txt       # Python dependencies
```

---

## 🧪 Training Pipeline – Step by Step

### 1. Set up your environment
```bash
python -m venv .venv
.venv\Scripts\activate  # on Windows
pip install -r requirements.txt
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
```

### 2. Get a Mapillary token
- Visit [Mapillary Developers](https://www.mapillary.com/dashboard/developers)
- Register your application and allow "READ" access
- Copy your API token and place it in [`.env`](downloader/.env):
```
MAPILLARY_TOKEN=MLY|YOUR|TOKEN # set your token here
```

<details><summary>📸 Click to show Mapillary screenshots</summary>

<br>

![register application](.doc/snapshot_mapillary_register_application.jpg)
![token](.doc/snapshot_mapillary_token.jpg)

</details>

### 3. Add data sources

- Add coordinates to: [`coordinates.txt`](downloader/download_by_area/coordinates.txt)
- Add sequence IDs to: [`sequences.txt`](downloader/download_by_area/sequences.txt)

### 4. Prepare the dataset 
```bash
python pipelines/prepare_data.py
```

### 5. Manually verify labels
Use [Labelme](https://github.com/wkentaro/labelme) to check/adjust bounding boxes.

### 6. Finalize the dataset
```bash
python pipelines/finalize_dataset.py
```

### 7. Augmentation
```bash
python preprocessing/split_aug_dataset.py
```

### 8. Train the model (You can enter your own parameters)
```bash
python training/train.py
```

> [!NOTE]  
> Training logs (metrics, artifacts) are saved to MLflow automatically if MLflow is installed and enabled in `train.py`.


### 9. Export the model (optional)
```bash
yolo export model=best.pt format=onnx
```

---

## 📂 Detailed description of the workflow

Mapillary is a platfrom to download and upload dashcam imagery.

This workflow allows you to download images either by area (default = 0.005 geographical degrees)
or by sequences (car routes).

### 📥 Download and Label Images:
### [`prepare_data.py`](pipelines/prepare_data.py)

This script filters out unlabeled images and creates the `dataset_prepared/` folder
with all labeled images in JSON format.

The next step is to manually verify the labels, since they are generated by a pretrained model.

> [!IMPORTANT]
> How do you that?\
> Use [Labelme](https://github.com/wkentaro/labelme)

When everything is verified, you can go to the next step.

---

### 🧹 Prepare YOLO Format:
### [`finalize_dataset.py`](pipelines/finalize_dataset.py)

After running [`finalize_dataset.py`](pipelines/finalize_dataset.py),
the dataset is stored in a YOLO-compatible format and structure:
```bash
dataset/
├── images/
│   ├──1.jpg
│   ├──2.jpg
│   ├──3.jpg
│   ...
│
└── labels/
    ├──1.txt
    ├──2.txt
    ├──3.txt
    ...
```

---

### ✨ Split & Augment
### [`split_aug_dataset.py`](preprocessing/split_aug_dataset.py)

The dataset is split into train/val/test sets (default: 80/20/0).
You can change this ratio in the script.

Built-in YOLO11 augmentations (e.g., flipLR, mosaic) are disabled
for full control and consistency.

Instead, augmentations are applied explicitly using Albumentations.

This makes experiments reproducible and lets you preview augmented images.

Example augmentation:
```python
from albumentations import (
    Compose, RandomBrightnessContrast,
    GaussianBlur, MotionBlur, OneOf, GaussNoise,
    HueSaturationValue, RandomRain, RandomShadow
)

transform = Compose([
    OneOf([
        MotionBlur(blur_limit=5),
        GaussianBlur(blur_limit=(3, 5)),
    ], p=0.4),

    OneOf([
        RandomRain(...),
        RandomShadow(...)
    ], p=0.4),
    
    GaussNoise(std_range=(0.05, 0.1), p=0.2),
    RandomBrightnessContrast(...),
    HueSaturationValue(...),
])
```

The structure after running [`split_aug_dataset.py`](./preprocessing/split_aug_dataset.py)
```bash
dataset/
├── images/
├── labels/
├── train/
│   ├── images/
│   └── labels/
├── val/
│   ├── images/
│   └── labels/
└── test/
    ├── images/
    └── labels/
```

> [!TIP]
> If you modify or extend your dataset, delete only the `train/`, `val/`, and `test/` folders.

---

### 🏋️Train with YOLO11
### [`train.py`](training/train.py)

Default parameters:
```python
params = {
    "cfg": "custom_args.yaml",
    "data": "data.yaml",
    "epochs": 100,
    "batch": 20,
    "imgsz": 640,
    "patience": 10,
    "device": "cuda",
    "augment": False,
    "project": "runs/detect",
    "name": "exp_yolo_mlflow",
    "exist_ok": False
}
```

> [!WARNING]
> TODO

---

## 🧰 Bonus [utils/](./utils) folder

<details>
    <summary>Click here to see bonus scripts</summary>

### [`count_instances_of_classes.py`](utils/count_instances_of_classes.py)
    
Generates class distribution chart from dataset_prepared

![class instances](.doc/class_instances_split_top40.png)

---

### [`decrease_image_resolution.py`](utils/decrease_image_resolution.py)

Preview how an image looks at YOLO input resolution.

Remember to put an `image.jpg` to utils folder.

---

### [`find_labels.py`](utils/find_labels.py)

Search for images with specific classes.
```python
targeted_classes = {'A-5'}
```
The script outputs names of jsons with found labels

---

### [`label_rare_classes.py`](utils/label_rare_classes.py) and [`label_frequent_classes.py`](utils/label_frequent_classes.py)
```python
# Rare
skip_classes = {"A-7", "B-33", ...}
```
```python
# Frequent
target_classes = {"A-7", "B-33", ...}
```

Why do I use both?

    1. Use label_rare_classes.py to skip classes that occur frequently.
    2. Delete images with no labels.
    3. Then run label_frequent_classes.py to auto-label those classes.
    4. Correct model errors in [Labelme](https://github.com/wkentaro/labelme).

</details>

---

## 📊 Final results (imgsz=640)

| Metric       | Value |
|--------------|-------|
| mAP@0.5      | 0.93  |
| mAP@0.5:0.95 | 0.78  |
| Precision    | 0.91  |
| Recall       | 0.86  |

## 🎯 Class-wise results:
### [`results_ultimo.txt`](.doc/results_ultimo.txt)

     Class     Images  Instances      Box(P          R      mAP50  mAP50-95):

       all       1883       3654      0.911      0.855      0.926      0.782
       A-1          6          6      0.857          1      0.995      0.841
       A-2          6          7          1      0.783      0.995      0.903
       ...         ...       ...        ...        ...        ...        ...

## 📈 Loss and metric curves:

![results](.doc/results.png)

---

Author: [Artur Sierakowski](https://github.com/ArturSierakowski)\
Source repository: [traffic-sign-detection-workflow](https://github.com/ArturSierakowski/traffic-sign-detection-workflow)
